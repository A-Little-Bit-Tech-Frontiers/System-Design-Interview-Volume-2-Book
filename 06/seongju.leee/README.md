## 0. 요구사항  

- 지난 M분 동안의 ad_id 클릭 수 집계
- 매분 가장 많이 클릭된 상위 100개 광고 아이디를 반환
- 다양한 속성에 따른 집계 필터링을 지원
- 데이터의 양은 페이스북이나 구글 규모



## 1. 데이터 모델

### 1.1 원시 데이터

아래는 로그 파일에 포함된 원시 데이터의 사례다.  
`[AdClickEvent] ad001, 2024-07-09 00:00:01, user-1, 207.148.22.22, USA`  

이런 데이터를 구조화된 형식으로 표현하면 아래 표와 같은 형태가 된다. 이런 데이터가 여러 애플리케이션 서버에 산재해 있는 것이다.  

| ad_id | timestamp | user_id | ip | country |
| --- | --- | --- | --- | --- |
| ad001 | 2024-07-09 00:00:01 | user-1 | 207.148.22.22 | USA |
| ad001 | 2024-07-09 00:00:02 | user-1 | 207.148.22.22 | USA |
| ad001 | 2024-07-09 00:00:02 | user-2 | 209.153.56.11 | USA |


### 1.2 집계 결과 데이터  
아래 표는 광고 클릭 이벤트가 매분 집계된다고 가정하였을 때의 집계 결과다.

| ad_id | click_minute | count |
| --- | --- | --- |
| ad001 | 202407090000 | 5 |
| ad001 | 202407090001 | 7 |


### 1.3 비교 

윈시 데이터와 집계 결과 데이터 모두 저장하는게 가장 좋은 방식이라고 볼 수 있다. 이유는 다음과 같다.

- 문제가 발생하면 디버깅에  활용할 수 있도록 원시 데이터도 보관하는 것이 좋다. 버그로 집계 데이터가 손상되면 버그 수정 후에 원시 데이터에서 집계 결과를 다시 만들 수 있다.
- 원시 데이터는 양이 엄청나므로 직접 질의하는 것은 비효율적이다.
- 원시 데이터는 백업 데이터로 활용된다. 재계산을 하는 경우가 아니라면 일반적으로 원시 데이터를 질의할 필요는 없다. 오래된 원시 데이터는 cold storage로 옮기면 비용 절감이 가능하다.

<br>

## 2. 올바른 데이터베이스 선택

올바른 데이터베이스를 선택하려면 다음과 같은 사항을 평가해 보아야 한다.

- 데이터는 어떤 모습인가? 관계형 데이터인가? 문서 데이터인가? 아니면 이진 대형 객체(Binary Large Object, BLOB)인가?
- 작업 흐름이 읽기 중심인가 쓰기 중심인가? 아니면 둘 다인가?
- 트랜잭션을 지원해야 하는가?
- 질의 과정에서 SUM이나 COUNT 같은 온라인 분석 처리(OLAP) 함수를 많이 사용해야 하는가?

평균 쓰기 QPS는 10,000이고, 최대 QPS는 50,000이다. 따라서 이 시스템은 **쓰기 중심 시스템**
원시 데이터는 백업과 재계산 용도로만 이용되므로 이론적으로는 읽기 연산 빈도는 낮다. RDB로도 할 수는 있겠으나 이 정도 규모의 쓰기 연산이 가능하도록 구성하기는 어렵다.  
쓰기 및 시간 범위 질의에 최적화된 카산드라나 InfluxDB를 사용하는 것이 좀 더 바람직하다.

집계 데이터는 본질적으로 시계열 데이터이며 이 데이터를 처리하는 워크플로는 **읽기 연산과 쓰기 연산을 둘 다 많이 사용**한다. 각 광고에 대해 매 분마다 데이터베이스에 질의를 던져 고객에게 최신 집계 결과를 제시해야 하기 때문이다.   
총 200만 개의 광고가 있으므로, 읽기 연산이 많이 발생할 수 밖에 없다. 집계 서비스가 데이터를 매 분 집계하고 그 결과를 기록하므로 쓰기 작업도 아주 빈번하게 이루어진다.  
원시 데이터와 집계 결과 데이터를 저장하는데는 같은 유형의 데이터베이스를 활용하는 것이 가능하다.

<br>


## 3. 개략적 설계안

<img width="667" alt="image" src="https://github.com/A-Little-Bit-Tech-Frontiers/System-Design-Interview-Volume-2-Book/assets/67941526/6f53ab68-cdf8-4e55-b9b0-1a049ecbc9e2">

실시간으로 빅데이터를 처리할 때 데이터는 보통 무제한으로 시스템에 흘러 들어왔다가 흘러 나간다. 집계 서비스도 마찬가지다. 다만 **입력은 원시 데이터(무제한 데이터 스트림)이고, 출력은 집계 결과**다.  

<br>

### 3.1 비동기 처리

지금과 같은 **동기식 데이터 처리는 생산자와 소비자 용량이 항상 같아야 한다.** **트래픽이 갑자기 증가하여 발생하는 이벤트 수가 소비자의 처리 용량을 훨씬 넘어서는 경우, 소비자는 메모리 부족 오류 등의 예기치 않은 문제를 겪게 될 수 있다. 동기식 시스템의 경우, 특정 컴포넌트의 장애는 전체 시스템 장애로 이어진다.**  
이 문제를 해결하기 위해 일반적으로 **생산자와 소비자의 결합을 끊는다. 그 결과로 전체 프로세스는 비동기 방식으로 동작하게 되고, 생산자와 소비자의 규모를 독립적으로 확장해 나갈 수 있게 된다.**

<img width="667" alt="image" src="https://github.com/A-Little-Bit-Tech-Frontiers/System-Design-Interview-Volume-2-Book/assets/67941526/5f5fefcb-9ee8-4a09-b40a-db96299c0990">

로그 감시자, 집계 서비스, 데이터베이스는 두 개의 메시지 큐로 분리되어 있다. 데이터베이스 기록 프로세스는 메시지 큐에서 데이터를 꺼내 데이터베이스가 지원하는 형식으로 변환한 다음 기록하는 역할을 수행한다.  


**왜 집계 결과를 데이터베이스에 바로 기록하지 않았을까?** **exactly once를 보장하기 위해서다. 원자적 커밋을 처리하기 위해 카프카 같은 시스템을 두 번째 메시지 큐로 도입**해야 하기 때문이다. 아래와 같다.

<img width="675" alt="image" src="https://github.com/A-Little-Bit-Tech-Frontiers/System-Design-Interview-Volume-2-Book/assets/67941526/6c117de7-5727-4cd2-987b-5b58f1817c52">

<br>

## 4. 집계 서비스

광고 클릭 이벤트를 **집계하는 좋은 방안 하나는 맵리듀스 프레임워크**를 사용하는 것이다. 맵리듀스 프레임워크에 좋은 모델은 유향 비순환 그래프(Directed Acyclic Graph, DAG)다. DAG 모델의 핵심은 아래와 같이 시스템을 맵/집계/리듀스 노드 등의 작은 컴퓨팅 단위로 세분화하는 것이다.  

<img width="486" alt="image" src="https://github.com/A-Little-Bit-Tech-Frontiers/System-Design-Interview-Volume-2-Book/assets/67941526/339a67ba-011b-4e12-9561-9be950e19bd6">

### 4.1 맵 노드

- 입력 데이터를 정리하거나 정규화해야 하는 경우에 필요
- 카프카 파티션을 사용해도 비슷하게 처리가 가능하지만, 데이터가 생성되는 방식에 대한 제어권이 없는 경우에는 동일한 ad_id를 갖는 이벤트가 서로 다른 카프카 파티션에 입력될 수도 있다는 점에서 차이가 존재함

### 4.2 집계 노드
- 집계 노드는 ad_id별 광고 클릭 이벤트 수를 매 분 메모리에서 집계
- 맵리듀스 패러다임에서 사실 집계 노드는 리듀스 프로세스의 일부다. 따라서 맵-집계-리듀스 프로세스는 실제로 맵-리듀스-리듀스 프로세스라고도 할 수 있다.

### 4.3 리듀스 노드

<img width="586" alt="image" src="https://github.com/A-Little-Bit-Tech-Frontiers/System-Design-Interview-Volume-2-Book/assets/67941526/1598377e-0801-409b-bb09-e7c57f8bc1bb">

- 모든 ‘집계’ 노드가 산출한 결과를 최종 결과로 축약

DAG는 맵리듀스 패러다임을 표현하기 위한 모델이다. 
빅데이터를 입력으로 받아 병렬 분산 컴퓨팅 자원을 활용하여 빅데이터를 작은, 또는 일반적 크기 데이터로 변화할 수 있도록 설계된 모델이다.

<br>

### 4.4 사례  

**4.4.1 가장 많이 클릭된 상위 N개 광고 반환**  

<img width="657" alt="image" src="https://github.com/A-Little-Bit-Tech-Frontiers/System-Design-Interview-Volume-2-Book/assets/67941526/58b6551f-4ff4-489c-8066-5f44b9e5238d">

- 가장 많이 클릭된 상위 광고 3개를 가져오는 방법의 단순화된 설계안이다. 이 방안은 상위 N개 광고로도 확장 가능
- **입력 이벤트는 `ad_id`를 기준으로 분배되고 각 집계 노드는 힙을 내부적으로 사용하여 상위 3개 광고를 효율적으로 식별**
- 마지막 단계의 리듀스 노드는 전달 받은 9개의 광고 가운데 지난 1분간 가장 많이 클릭된 광고 3개를 골라냄

## 5. 집계 서비스의 규모 확장

<img width="751" alt="image" src="https://github.com/A-Little-Bit-Tech-Frontiers/System-Design-Interview-Volume-2-Book/assets/67941526/844710d3-d401-48de-a458-bd93dff02e31">

1. 분배(맵) 연산은 데이터를 분할하여 여러 작업 단위로 나눔 (e.g. `ad_id%2` , `ad_id%3` , ...)
2. 중간 단계인 분배 연산 결과는 각 집계 서버에서 나온 데이터를 다른 집계 서버로 전송 (이때, 이 결과의 형식은 `ad_1: 2` , ..)
3. 분배된 연산 결과들을 종합하기 위해 축약(리듀스) 연산 입력에서 데이터를 입력 받음 (이때, 각 집계 서버별 동일한 키를 가진 데이터를 입력 받음)
4. 입력받은 분산 결과들을 하나로 종합하는 축약 프로세스를 거쳐 최종 연산 결과를 도출

집계 서비스는 본질적으로 위와 같이 맵리듀스 연산으로 구현  
- 노드의 추가/삭제를 통해 집계 서비스 수평적 규모 확장 가능
